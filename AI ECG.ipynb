{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEA\n",
    "\n",
    "- Usar una Inception Network y concatenarla con una Dense Layer que procesa la informacion del paciente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import wfdb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import ast\n",
    "from wfdb import processing\n",
    "import os\n",
    "from scipy import signal\n",
    "import scipy\n",
    "import matplotlib as mpl\n",
    "from ssqueezepy import cwt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing W&B package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wandb in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (0.16.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (49.6.0.post20200814)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (1.39.1)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.10\" in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (4.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (2.27.1)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0; sys_platform != \"linux\" in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (3.19.4)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (3.1.40)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from wandb) (5.9.7)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.3)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (4.12.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from sentry-sdk>=1.0.0->wandb) (2022.9.24)\n",
      "Requirement already satisfied: urllib3>=1.26.11; python_version >= \"3.6\" in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from sentry-sdk>=1.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->Click!=8.0.0,>=7.1->wandb) (3.1.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\erno\\anaconda3\\envs\\torchenv\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Erno/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PTB-XL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example_physionet.py',\n",
       " 'LICENSE.txt',\n",
       " 'ptbxl_database.csv',\n",
       " 'RECORDS',\n",
       " 'records100',\n",
       " 'records500',\n",
       " 'scp_statements.csv',\n",
       " 'SHA256SUMS.txt',\n",
       " 'wandb']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('F:\\Datasets\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1AVB', '2AVB', '3AVB', 'ABQRS', 'AFIB', 'AFLT', 'ALMI', 'AMI',\n",
       "       'ANEUR', 'ASMI', 'BIGU', 'CLBBB', 'CRBBB', 'DIG', 'EL', 'HVOLT',\n",
       "       'ILBBB', 'ILMI', 'IMI', 'INJAL', 'INJAS', 'INJIL', 'INJIN',\n",
       "       'INJLA', 'INVT', 'IPLMI', 'IPMI', 'IRBBB', 'ISCAL', 'ISCAN',\n",
       "       'ISCAS', 'ISCIL', 'ISCIN', 'ISCLA', 'ISC_', 'IVCD', 'LAFB',\n",
       "       'LAO/LAE', 'LMI', 'LNGQT', 'LOWT', 'LPFB', 'LPR', 'LVH', 'LVOLT',\n",
       "       'NDT', 'NORM', 'NST_', 'NT_', 'PAC', 'PACE', 'PMI', 'PRC(S)',\n",
       "       'PSVT', 'PVC', 'QWAVE', 'RAO/RAE', 'RVH', 'SARRH', 'SBRAD',\n",
       "       'SEHYP', 'SR', 'STACH', 'STD_', 'STE_', 'SVARR', 'SVTAC', 'TAB_',\n",
       "       'TRIGU', 'VCLVH', 'WPW'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCP_DATA = pd.read_csv(os.path.join(os.getcwd(), 'scp_statements.csv'))\n",
    "scp_codes = np.unique(SCP_DATA['Unnamed: 0'].to_numpy())\n",
    "scp_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "METADATA = pd.read_csv(os.path.join(os.getcwd(), 'ptbxl_database.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CD', 'HYP', 'MI', 'NORM', 'STTC'], dtype='<U4')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostic_class = np.unique([i for i in SCP_DATA['diagnostic_class'].to_numpy() if i is not np.nan])\n",
    "diagnostic_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AMI', 'CLBBB', 'CRBBB', 'ILBBB', 'IMI', 'IRBBB', 'ISCA', 'ISCI',\n",
       "       'ISC_', 'IVCD', 'LAFB/LPFB', 'LAO/LAE', 'LMI', 'LVH', 'NORM',\n",
       "       'NST_', 'PMI', 'RAO/RAE', 'RVH', 'SEHYP', 'STTC', 'WPW', '_AVB'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnostic_subclass = np.unique([i for i in SCP_DATA['diagnostic_subclass'].to_numpy() if i is not np.nan])\n",
    "diagnostic_subclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning label to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scp_code2id = {}\n",
    "for i, code in enumerate(scp_codes):\n",
    "    scp_code2id[code] = i\n",
    "\n",
    "id2scp_code = {}\n",
    "for i, code in enumerate(scp_codes):\n",
    "    id2scp_code[i] = code\n",
    "    \n",
    "len(id2scp_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wandb preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    epochs=10,\n",
    "    classes=len(id2scp_code),\n",
    "#     kernels=[16, 32],\n",
    "    batch_size=16,\n",
    "    learning_rate=0.1,\n",
    "    architecture=\"Own-ResNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.lib.preinit.PreInitObject at 0x21e351ba788>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_one_hot(idx):\n",
    "    x = np.zeros(len(scp_codes))\n",
    "    x[idx] = 1\n",
    "    return x\n",
    "\n",
    "str2dict = lambda x: ast.literal_eval(x)\n",
    "signal_x = 0\n",
    "\n",
    "signals = []\n",
    "labels = []\n",
    "\n",
    "for i in range(int(METADATA.shape[0] * .4)): # METADATA.shape[0]\n",
    "    raw_data = METADATA.iloc[i]\n",
    "    \n",
    "    file_path = os.path.join(os.getcwd(), *raw_data.filename_lr.split('/'))  \n",
    "    diagnostic_scp_codes = str2dict(raw_data.scp_codes)\n",
    "    \n",
    "    signal, signal_metadata = wfdb.rdsamp(file_path)\n",
    "    channels = signal_metadata['sig_name']\n",
    "    \n",
    "    # Patience data\n",
    "    age = raw_data.age if raw_data.age is not np.nan else raw_data.age\n",
    "    sex = raw_data.sex if raw_data.sex is not np.nan else raw_data.sex\n",
    "    weight = raw_data.weight if raw_data.weight is not np.nan else raw_data.weight\n",
    "    \n",
    "    # Normalization\n",
    "    channel_max = np.max(signal, axis=0)\n",
    "    channel_min = np.min(signal, axis=0)\n",
    "\n",
    "    num_channels = signal.shape[-1]\n",
    "    for i in range(num_channels):\n",
    "        signal[:,i] = (signal[:,i] - channel_min[i]) / (channel_max[i] - channel_min[i])\n",
    "    signal_x = signal\n",
    "    \n",
    "    # Agarro el que mayor porcentaje de ser tiene\n",
    "    pc = list(diagnostic_scp_codes.values())\n",
    "    max_pc_idx = pc.index(max(pc))\n",
    "    label = list(diagnostic_scp_codes.keys())[max_pc_idx]\n",
    "    label = scp_code2id[label]\n",
    "    \n",
    "    signals.append(signal_x)\n",
    "#     labels.append(encode_one_hot(label))\n",
    "    labels.append(label)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8734, 12, 1000]), torch.Size([8734]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor(np.array(signals))\n",
    "X = X.permute(0,2,1)\n",
    "\n",
    "# y = torch.tensor(np.array(labels))\n",
    "y = torch.tensor(np.array(labels))#.reshape(-1,1)\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_testing_data(X, y, bz=16):\n",
    "    train_size = int(.8 * .4 * METADATA.shape[0])\n",
    "    # train_size = int(.9 * 200)\n",
    "    X_train, X_test = X[:train_size], X[train_size:]\n",
    "    y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=bz)\n",
    "\n",
    "    test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=bz)\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "    \n",
    "# create_training_testing_data(X, y, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, ksize=3, stride=2):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, hidden_channels, ksize, stride=stride),\n",
    "            nn.BatchNorm1d(hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout1d(.3),\n",
    "            nn.Conv1d(hidden_channels, out_channels, ksize, stride=stride),\n",
    "            nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        block_out = self.block(x)\n",
    "#         print(\"Block Out | Shape {}\".format(block_out.shape))\n",
    "        return nn.ReLU()(x + block_out)\n",
    "\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, ksize=3, stride=2):\n",
    "#         NeuralNetwork.__init__(self, in_channels, hidden_channels, out_channels)\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "#         NeuralNetwork.__init__(self, in_channels, hidden_channels, out_channels, ksize=3, stride=2)\n",
    "        self.in_model = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, hidden_channels, 3, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_channels),\n",
    "            nn.MaxPool1d(ksize),\n",
    "#             nn.AvgPool1d(ksize)\n",
    "        )\n",
    "        self.res1 = ResidualBlock(hidden_channels, 3, hidden_channels, ksize, stride)\n",
    "        \n",
    "        self.out_model = nn.Sequential(\n",
    "            nn.MaxPool1d(ksize),\n",
    "            nn.Flatten(),  \n",
    "        )\n",
    "       \n",
    "    def forward(self, x):\n",
    "        x = self.in_model(x)\n",
    "#         print(\"Stage 1 | Shape: {}\".format(x.shape))\n",
    "        x = self.res1(x)\n",
    "#         print(\"Stage 2 | Shape: {}\".format(x.shape))\n",
    "        x = self.out_model(x)\n",
    "#         print(\"Stage 3 | Shape: {}\".format(x.shape))\n",
    "        \n",
    "        out = nn.Sequential(\n",
    "            nn.Linear(x.shape[-1], len(scp_codes)),\n",
    "            nn.Softmax(dim=1)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        x = out(x)\n",
    "#         print(\"Stage 4 | Shape: {}\".format(x.shape))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralNetwork(12, 6, 12, 1, 1)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, acc, epoch):\n",
    "    # Where the magic happens\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": loss, \"accuracy\": acc})\n",
    "#     print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_dataloader, epochs=10):\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        wandb.watch(model, criterion, log=\"all\", log_freq=1)\n",
    "\n",
    "        loss_values = []\n",
    "        acc_values = []\n",
    "\n",
    "        for signals, labels in train_dataloader:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            pred = model(signals.float())\n",
    "            loss = criterion(pred, labels.long())\n",
    "            loss_values.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pred_indeces = torch.argmax(pred, 1)\n",
    "            acc = torch.sum(pred_indeces == labels) / pred_indeces.shape[0]\n",
    "            acc_values.append(acc)\n",
    "            \n",
    "            train_log(loss, acc, epoch)\n",
    "\n",
    "        total_loss.append(sum(loss_values) / len(loss_values))\n",
    "        total_acc.append(sum(acc_values) / len(acc_values))\n",
    "\n",
    "        print(\"Epoch: {} | Loss: {} | Accuracy: {}\".format(epoch, total_loss[-1], total_acc[-1]))\n",
    "\n",
    "    print(\"Training Complete\")\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance(loss_values, acc_values):\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(14, 11), sharex=True)\n",
    "    axs[0].plot(loss_values)\n",
    "    axs[0].grid(True)\n",
    "    axs[0].set_ylabel(\"Loss\", size=24)\n",
    "\n",
    "    axs[1].plot(acc_values)\n",
    "    axs[1].grid(True)\n",
    "    axs[1].set_ylabel(\"Accuracy\", size=24)\n",
    "    axs[1].set_xlabel(\"Epoch\", size=24)\n",
    "    \n",
    "    \n",
    "# plot_performance(total_loss, total_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model():\n",
    "    total_loss = []\n",
    "    total_acc = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        loss_values = []\n",
    "        acc_values = []\n",
    "\n",
    "        for signals, y in test_dataloader:\n",
    "            outputs = model(signals.float())\n",
    "            loss = loss_fn(outputs, y.long())\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            pred_indeces = torch.argmax(outputs, 1)\n",
    "            acc = torch.sum(pred_indeces == y) / pred_indeces.shape[0]\n",
    "            acc_values.append(acc)\n",
    "\n",
    "            total_loss.append(sum(loss_values) / len(loss_values))\n",
    "            total_acc.append(sum(acc_values) / len(acc_values))\n",
    "\n",
    "        print(\"Epoch: {} | Loss: {} | Accuracy: {}\".format(epoch, total_loss[-1], total_acc[-1]))\n",
    "\n",
    "\n",
    "    print(\"Testing finished!\")\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_performance(total_loss, total_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make():\n",
    "    model = NeuralNetwork(12, 6, 12, 1, 1)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=0.01)\n",
    "    \n",
    "    train_dataloader, test_dataloader = create_training_testing_data(X, y, 32)\n",
    "    \n",
    "    return model, loss_fn, optimizer, train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>F:\\Datasets\\ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.1\\wandb\\run-20231229_025909-piev6fib</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ernestosegundo/ECG/runs/piev6fib' target=\"_blank\">toasty-frog-5</a></strong> to <a href='https://wandb.ai/ernestosegundo/ECG' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ernestosegundo/ECG' target=\"_blank\">https://wandb.ai/ernestosegundo/ECG</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ernestosegundo/ECG/runs/piev6fib' target=\"_blank\">https://wandb.ai/ernestosegundo/ECG/runs/piev6fib</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 4.262819784417 | Accuracy: 0.01397104561328888\n",
      "Epoch: 1 | Loss: 4.262763258529036 | Accuracy: 0.014840182848274708\n",
      "Epoch: 2 | Loss: 4.262728283938752 | Accuracy: 0.016124429181218147\n",
      "Epoch: 3 | Loss: 4.262781356567661 | Accuracy: 0.010844748467206955\n",
      "Epoch: 4 | Loss: 4.262644465111162 | Accuracy: 0.012699771672487259\n",
      "Epoch: 5 | Loss: 4.262615410704591 | Accuracy: 0.013958074152469635\n",
      "Epoch: 6 | Loss: 4.2627794426870125 | Accuracy: 0.01141552533954382\n",
      "Epoch: 7 | Loss: 4.2627181897969 | Accuracy: 0.012985159642994404\n",
      "Epoch: 8 | Loss: 4.262692745417764 | Accuracy: 0.012414383701980114\n",
      "Epoch: 9 | Loss: 4.262692377447538 | Accuracy: 0.013542963191866875\n",
      "Training Complete\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef30c771cf9448c90c93c6dd01618e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    }
   ],
   "source": [
    "def make_pipeline():\n",
    "    with wandb.init(project=\"ECG\"):\n",
    "        model, loss_fn, optimizer, train_dataloader, test_dataloader = make()\n",
    "        epochs = 10\n",
    "        \n",
    "        train_model(model, loss_fn, optimizer, train_dataloader, epochs)\n",
    "        \n",
    "#     return model\n",
    "make_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "y_pred = list(itertools.chain(*y_pred))\n",
    "y_test = list(itertools.chain(*y_test))\n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.subplots(figsize=(8, 5))\n",
    "\n",
    "sns.heatmap(cf_matrix, annot=True, cbar=False, fmt=\"g\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
